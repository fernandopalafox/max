{
    "env_name": "pursuit_evasion",
    "env_params": {
        "num_agents": 3,
        "box_half_width": 1.0,
        "max_episode_steps": 25,
        "dt": 0.1,
        "max_accel": 2.0,
        "pursuer_max_accel": 3.0,
        "evader_max_accel": 4.0,
        "pursuer_max_speed": 1.0,
        "evader_max_speed": 1.3,
        "pursuer_size": 0.075,
        "evader_size": 0.05,
        "reward_shaping_k1": 1.0,
        "reward_shaping_k2": 1.0,
        "reward_collision_penalty": 1.0
    },
    "total_steps": 200000,
    "num_agents": 3,
    "dim_state": 14,
    "dim_action": 2,
    "train_freq": 1,
    "train_policy_freq": 2048,
    "normalize_freq": 1000000,
    "eval_freq": 100,
    "eval_traj_horizon": 100,
    "normalization": {
        "method": "static"
    },
    "normalization_params": {
        "state": {
            "min": [
                -1.0,
                -1.0,
                -1.5,
                -1.5,
                -1.0,
                -1.0,
                -1.5,
                -1.5,
                -1.0,
                -1.0,
                -1.5,
                -1.5,
                -1.0,
                -1.0
            ],
            "max": [
                1.0,
                1.0,
                1.5,
                1.5,
                1.0,
                1.0,
                1.5,
                1.5,
                1.0,
                1.0,
                1.5,
                1.5,
                1.0,
                1.0
            ]
        },
        "action": {
            "min": [
                -2.0,
                -2.0
            ],
            "max": [
                2.0,
                2.0
            ]
        }
    },
    "policy": "actor-critic",
    "policy_params": {
        "hidden_layers": [
            64,
            64
        ]
    },
    "policy_trainer": "ippo",
    "policy_trainer_params": {
        "actor_lr": 0.0003,
        "critic_lr": 0.001,
        "ppo_lambda": 0.95,
        "ppo_gamma": 0.99,
        "clip_epsilon": 0.2,
        "n_epochs": 4,
        "mini_batch_size": 64,
        "entropy_coef": 0.01,
        "value_coef": 0.5,
        "max_grad_norm": 0.5
    },
    "policy_evaluator_params": {
        "n_episodes": 10
    },
    "reward_scaling_discount_factor": 0.99,
    "reward_clip": 100.0
}